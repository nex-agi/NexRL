apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{JOB_NAME}}-rollout
  namespace: {{NAMESPACE}}
  labels:
    app: nexrl-rollout-sglang
    model: {{SERVED_MODEL_NAME}}
    job-name: {{JOB_NAME}}
    nexrl-job-type: rollout-workers
spec:
  replicas: {{INFERENCE_REPLICAS|default(1)}}
  selector:
    matchLabels:
      app: nexrl-rollout-sglang
  template:
    metadata:
      labels:
        app: nexrl-rollout-sglang
        model: {{SERVED_MODEL_NAME}}
        job-name: {{JOB_NAME}}
        nexrl-job-type: rollout-workers
    spec:
      containers:
      - name: rollout-server
        image: {{INFERENCE_IMAGE}}
        imagePullPolicy: IfNotPresent
        command:
          - /bin/bash
          - -c
          - "sed -i 's/for logprob, _, token_text in token_logprobs:/for logprob, token_id, token_text in token_logprobs:/g' /sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/utils.py && sed -i 's/ret_logprobs.tokens.append(token_text)/ret_logprobs.tokens.append(str(token_id))/g' /sgl-workspace/sglang/python/sglang/srt/entrypoints/openai/utils.py && python -m sglang.launch_server --model-path \"{{MODEL_PATH}}\" --trust-remote-code --served-model-name \"{{SERVED_MODEL_NAME}}\" --host \"0.0.0.0\" --port \"8000\" --tp-size \"{{TP_SIZE|default(1)}}\"{% if ADDITIONAL_ARGS %} {{ADDITIONAL_ARGS}}{% endif %}"
        ports:
          - containerPort: 8000
            name: http
            protocol: TCP
        env:
          - name: NCCL_DEBUG
            value: "INFO"
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: nexrl-redis-secret
                key: REDIS_HOST
          - name: REDIS_PORT
            valueFrom:
              secretKeyRef:
                name: nexrl-redis-secret
                key: REDIS_PORT
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                name: nexrl-redis-secret
                key: REDIS_PASSWORD
        resources:
          requests:
            nvidia.com/gpu: "{{INFERENCE_GPUS}}"
            memory: "{{INFERENCE_MEMORY|default('200Gi')}}"
          limits:
            nvidia.com/gpu: "{{INFERENCE_GPUS}}"
            memory: "{{INFERENCE_MEMORY|default('200Gi')}}"
        volumeMounts:
          - name: shared-storage
            mountPath: {{STORAGE_PATH}}
          - name: dshm
            mountPath: /dev/shm
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
      volumes:
        - name: shared-storage
          hostPath:
            path: {{STORAGE_PATH}}
            type: Directory
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: "32Gi"
