# On-Policy Distillation (OPD) Design Document

## Overview

On-Policy Distillation (OPD) is a technique for training a smaller **student** model to mimic a larger **teacher** model. Unlike traditional offline distillation, OPD uses trajectories generated by the student model itself (on-policy), with the teacher providing dense feedback through log probabilities.

### Key Benefits
- **Memory Efficient**: Uses log probabilities (~40,000x smaller than full logits for typical vocab sizes)
- **On-Policy Learning**: Student learns from its own distribution, avoiding distribution mismatch
- **Dense Feedback**: Teacher provides token-level guidance, not just final rewards

### Mathematical Foundation

The distillation loss minimizes the reverse KL divergence:

```
L_distill = KL(Ï€_student || Ï€_teacher) = E_x~Ï€_student [log Ï€_student(x) - log Ï€_teacher(x)]
```

This encourages the student to:
1. Assign high probability to tokens the teacher likes
2. Avoid tokens the teacher dislikes

---

## Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              NexRL Controller                                â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Loader â”‚â”€â”€â”€â–¶â”‚  Rollout Worker  â”‚â”€â”€â”€â–¶â”‚     Trajectory Pool         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  (Student Gen)   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚                  â”‚
â”‚                            â”‚                             â”‚                  â”‚
â”‚                            â–¼                             â–¼                  â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                   â”‚ Inference Serviceâ”‚         â”‚  SelfHostedOpdTrainer   â”‚  â”‚
â”‚                   â”‚ (Student sglang) â”‚         â”‚                         â”‚  â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  1. _prepare_batch()    â”‚  â”‚
â”‚                                                â”‚     - Get teacher logP  â”‚  â”‚
â”‚                                                â”‚  2. train()             â”‚  â”‚
â”‚                                                â”‚     - Distillation loss â”‚  â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                           â”‚                 â”‚
â”‚                                                           â–¼                 â”‚
â”‚                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                                â”‚      API Server         â”‚  â”‚
â”‚                                                â”‚  /update_actor_with_    â”‚  â”‚
â”‚                                                â”‚   distillation          â”‚  â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚           â”‚           â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                          â–¼                                â–¼ â”‚
â”‚                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                 â”‚ Student Workers â”‚            â”‚ Teacher Workers  â”‚
â”‚                                 â”‚ (nexrl-group-1) â”‚            â”‚ (nexrl-group-2)  â”‚
â”‚                                 â”‚ - Train student â”‚            â”‚ - Compute logP   â”‚
â”‚                                 â”‚ - Save ckpt     â”‚            â”‚ - No training    â”‚
â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. Data Loader â†’ Prompts
2. Rollout Worker â†’ Student generates responses (trajectories)
3. Trajectory Pool â†’ Batch trajectories
4. OPD Trainer:
   a. _prepare_batch():
      - Send batch to Teacher â†’ Get teacher_log_probs
      - Add teacher_log_probs to batch
   b. train():
      - Send batch to Student â†’ update_actor_with_distillation()
      - Compute distillation loss: KL(student || teacher)
      - Update student weights
5. Weight Sync â†’ Update inference service with new student weights
6. Repeat
```

---

## Implementation Details

### Files Created/Modified

| File | Status | Description |
|------|--------|-------------|
| `recipe/on_policy_distill/opd_qwen2a5_3b_from_14b.yaml` | âœ… Created | Config for 3B student, 14B teacher |
| `nexrl/trainer/self_hosted_opd_trainer.py` | âœ… Created | OPD trainer implementation |
| `nexrl/trainer/__init__.py` | âœ… Modified | Register SelfHostedOpdTrainer |
| `nexrl/controller.py` | âœ… Modified | Inject teacher_service config |

### Key Classes

#### SelfHostedOpdTrainer

```python
class SelfHostedOpdTrainer(SelfHostedTrainer):
    """
    Self-hosted trainer for On-Policy Distillation.

    Key methods:
    - initialize_workers(): Init both student and teacher workers
    - _prepare_batch(): Get teacher log probs, add to batch
    - train(): Call update_actor_with_distillation() instead of update_actor()
    """
```

#### Config Structure

```yaml
trainer:
  type: "self_hosted_opd"
  algorithm:
    type: "distillation"
    distillation_coeff: 1.0
    entropy_coeff: 0.0
    temperature: 0.6

service:
  train_service:      # Student training service
    url: "http://${API_SERVER_URL}:8000"
    actor:
      model:
        path: "${student_model_path}"

  teacher_service:    # Teacher service (new for OPD)
    identifier: "teacher"
    url: "http://${API_SERVER_URL}:8000"
    actor:
      model:
        path: "${teacher_model_path}"

resource:
  train:
    nexrl-group-1:    # Student workers
      world_size: 1
      gpus_per_pod: 8
    nexrl-group-2:    # Teacher workers
      world_size: 2
      gpus_per_pod: 8
```

---

## API Endpoints

### Existing Endpoints (Already Implemented)

| Endpoint | Purpose | Used By |
|----------|---------|---------|
| `/compute_log_prob` | Compute log probabilities | Teacher (get teacher_log_probs) |
| `/update_actor_with_distillation` | Train with distillation loss | Student (training step) |
| `/initialize` | Initialize worker group | Both student and teacher |
| `/init_model` | Load model to GPU | Both student and teacher |

### Client Methods

```python
# HTTPTrainServiceClient methods used by OPD:

# For teacher:
teacher_client.initialize_worker(config_dict, role="actor")
teacher_client.init_model()
teacher_client.compute_log_prob(batch)  # Get teacher log probs

# For student:
student_client.initialize_worker(config_dict, role="actor")
student_client.init_model()
student_client.update_actor_with_distillation(batch)  # Train with distillation
student_client.save_checkpoint(...)
```

---

## What's Done âœ…

### 1. Config File
- [x] Created `recipe/on_policy_distill/opd_qwen2a5_3b_from_14b.yaml`
- [x] Configured student model (Qwen2.5-3B)
- [x] Configured teacher model (Qwen2.5-14B)
- [x] Set up resource allocation for both student and teacher workers
- [x] Added distillation algorithm parameters

### 2. Trainer Implementation
- [x] Created `SelfHostedOpdTrainer` class
- [x] Implemented `initialize_workers()` to init both student and teacher
- [x] Implemented `_prepare_batch()` to get teacher log probs
- [x] Overrode `train()` to call `update_actor_with_distillation()`
- [x] Added distillation-specific metrics logging

### 3. Registration
- [x] Added `SelfHostedOpdTrainer` to `nexrl/trainer/__init__.py`
- [x] Added `"self_hosted_opd"` to trainer registry in `controller.py`
- [x] Added logic to inject `teacher_service` config into trainer

### 4. API Server (Pre-existing)
- [x] `/update_actor_with_distillation` endpoint exists
- [x] `/compute_log_prob` endpoint exists
- [x] `HTTPTrainServiceClient.update_actor_with_distillation()` method exists

---

## What's Next ğŸš§

### High Priority

#### 1. Verify Backend Worker Implementation
**Status**: âš ï¸ Needs Verification

The API server routes requests to backend workers. Need to verify:

```python
# In nextrainer/worker.py or similar:
def update_actor_with_distillation(self, data_proto):
    """
    Compute distillation loss and update actor.

    Expected data_proto fields:
    - input_ids, attention_mask, position_ids
    - teacher_log_probs  # From teacher model
    - scoring_attention_mask  # For loss masking

    Loss computation:
    - student_log_probs = model.forward(input_ids).log_softmax()
    - distill_loss = (student_log_probs - teacher_log_probs) * mask
    - loss = distill_loss.mean()  # or token-mean based on config
    """
```

**Action Items**:
- [ ] Check if `update_actor_with_distillation` is implemented in the worker
- [ ] Verify the loss computation matches the expected formula
- [ ] Ensure `teacher_log_probs` is correctly consumed

#### 2. Multi-Worker Group Support
**Status**: âš ï¸ Needs Verification

The API server needs to support multiple worker groups (student + teacher):

```python
# Expected behavior:
# 1. Student workers registered with identifier=None (default)
# 2. Teacher workers registered with identifier="teacher"
# 3. Requests routed based on identifier parameter
```

**Action Items**:
- [ ] Verify API server supports multiple `zmq_coordinators`
- [ ] Test that teacher workers can be initialized with `identifier="teacher"`
- [ ] Ensure GPU scheduling doesn't conflict between student and teacher

#### 3. End-to-End Testing
**Status**: ğŸ”´ Not Started

**Action Items**:
- [ ] Create a minimal test config (small models, few steps)
- [ ] Run end-to-end test on a single node
- [ ] Verify metrics are logged correctly
- [ ] Check checkpoint saving works

### Medium Priority

#### 4. CLI Support for OPD
**Status**: ğŸ”´ Not Started

The CLI scripts may need updates to handle OPD-specific resource allocation:

```bash
# Expected usage:
nexrl run recipe/on_policy_distill/opd_qwen2a5_3b_from_14b.yaml
```

**Action Items**:
- [ ] Review `cli/self_hosted/` scripts for multi-worker-group support
- [ ] Update Kubernetes job templates if needed
- [ ] Add documentation for running OPD experiments

#### 5. Validation During Training
**Status**: ğŸŸ¡ Partial

Current implementation validates using the student model. May want to add:
- Teacher-student agreement metrics
- KL divergence tracking over time

**Action Items**:
- [ ] Add `distill/kl_divergence` metric
- [ ] Add `distill/teacher_student_agreement` metric
- [ ] Consider adding teacher perplexity on student outputs

### Low Priority

#### 6. Advanced Features
- [ ] Support for multiple teachers (ensemble distillation)
- [ ] Curriculum learning (gradually increase teacher temperature)
- [ ] Selective distillation (only distill on certain tokens)
- [ ] Hybrid loss (distillation + RL reward)

#### 7. Documentation
- [ ] Add OPD section to main README
- [ ] Create tutorial notebook
- [ ] Document hyperparameter tuning guidelines

---

## Configuration Reference

### Distillation Algorithm Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `distillation_coeff` | 1.0 | Weight of distillation loss |
| `entropy_coeff` | 0.0 | Weight of entropy bonus (exploration) |
| `temperature` | 0.6 | Softmax temperature for distillation |
| `loss_agg_mode` | "token-mean" | How to aggregate loss ("token-mean", "seq-mean") |
| `distillation_epochs` | 1 | Number of epochs per batch |

### Resource Allocation Guidelines

| Model Size | Recommended GPUs | Memory per GPU |
|------------|------------------|----------------|
| 3B Student | 8 GPUs (1 pod) | 80GB |
| 7B Student | 8 GPUs (1 pod) | 80GB |
| 14B Teacher | 16 GPUs (2 pods) | 80GB |
| 32B Teacher | 32 GPUs (4 pods) | 80GB |

---

## Troubleshooting

### Common Issues

#### 1. Teacher Worker Initialization Failed
```
Error: Teacher worker initialization failed
```
**Solution**: Check that:
- `teacher_service.identifier` is set to a unique value (e.g., "teacher")
- `resource.train.nexrl-group-2` has sufficient resources
- API server logs for detailed error

#### 2. OOM on Teacher
```
Error: CUDA out of memory
```
**Solution**:
- Increase `teacher_service.actor.ppo_micro_batch_size`
- Increase `resource.train.nexrl-group-2.world_size`
- Enable `fsdp_config.param_offload: true`

#### 3. Slow Training
**Symptoms**: Training step takes too long
**Solution**:
- Check if teacher log prob computation is the bottleneck
- Consider using async mode for teacher computation
- Reduce `data.batch_size`

---

## References

- [On-Policy Distillation Blog](https://thinkingmachines.ai/blog/on-policy-distillation/)
- [Knowledge Distillation Survey](https://arxiv.org/abs/2006.05525)
- [NexRL Documentation](../docs/)

---

## Changelog

| Date | Author | Changes |
|------|--------|---------|
| 2025-01-20 | AI Assistant | Initial implementation of OPD trainer |
| | | Created config file for 3Bâ†’14B distillation |
| | | Registered trainer in controller |
