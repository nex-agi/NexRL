# Copyright (c) Nex-AGI. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

defaults:
  - common
  - _self_

project_name: "NexRL-self-hosted"
experiment_name: "self-hosted-nexau-news"

#==========================================
# ENVIRONMENT (Self-hosted-specific)
#==========================================

environment:
  setup_script: "self_hosted.env.sh"
  require_setup_script: true

#==========================================
# DATA CONFIGURATION (Self-hosted-specific)
#==========================================

data:
  # Override tokenizer to use model path
  tokenizer_path: ${service.inference_service.model_path}
  # Override max_prompt_length for rollout worker
  max_prompt_length: ${rollout_worker.max_prompt_length}

#==========================================
# TRAINER CONFIGURATION (Self-hosted-specific)
#==========================================

trainer:
  type: "self_hosted_grpo"

  # Self-hosted specific paths
  checkpoint_path: "${oc.env:EXPERIMENT_PATH}/ckpt"
  sync_weight_path: "${oc.env:EXPERIMENT_PATH}/sync_weight"
  remove_previous_ckpt: false

  algorithm:
    type: "grpo"
    do_old_log_prob_compute: true
    use_kl_in_reward: false
    kl_penalty: kl

    kl_ctrl:
      type: fixed
      kl_coef: 0.001
      kl_reward_coef: ${trainer.algorithm.kl_ctrl.kl_coef}

#==========================================
# WEIGHT MANAGEMENT (Self-hosted-specific)
#==========================================

weight:
  sync_method: "disk"
  sync_weight_path: ${trainer.sync_weight_path}

#==========================================
# SERVICE CONFIGURATION (Self-hosted-specific)
#==========================================

service:
  inference_service:
    identifier: "default"
    api_key: "EMPTY"
    base_url: "${oc.env:INFERENCE_BASE_URL}"
    model: "nexrl-nexau-news-qwen3-8b"
    model_path: "${oc.env:NEXRL_MODEL_PATH}/Qwen/Qwen3-8B"
    max_tokens: ${data.max_response_length}
    tokenizer: ${data.tokenizer_path}
    backend: sglang
    max_retries: 3
    freeze_for_weight_sync: true
    weight_type: "sglang_nckpt"

    resource:
      replicas: 4
      gpus_per_replica: 2
      backend: "sglang"
      extra_args: ""

  train_service:
    main_actor:
      identifier: "default"
      role: "actor"
      backend: http
      url: "http://${oc.env:API_SERVER_URL}:8000"

      resource:
        world_size: 1
        gpus_per_pod: 8
        memory_per_gpu: 200

      actor:
        checkpoint_manager: dcp

        model:
          path: "${service.inference_service.model_path}"
          use_remove_padding: true

        ppo_mini_batch_size: 16
        ppo_micro_batch_size: 2
        grad_clip: 1.0
        clip_ratio: 0.2
        clip_ratio_low: 0.2
        clip_ratio_high: 0.2
        clip_ratio_c: 3.0
        loss_agg_mode: token-mean
        loss_func_type: NX_20250515
        entropy_coeff: 1e-4
        use_kl_loss: false
        kl_loss_type: low_var_kl
        kl_loss_coef: 0.0
        use_ring_sequence_parallel: false
        ppo_epochs: 1
        ulysses_sequence_parallel_size: 4
        use_dynamic_bsz: false
        ppo_max_token_len_per_gpu: 16384
        do_old_log_prob_compute: true

        optim:
          lr: 2e-6
          lr_warmup_steps_ratio: 0.0
          warmup_style: constant

        fsdp_config:
          wrap_policy:
            min_num_params: 0
          param_offload: false
          grad_offload: false
          optimizer_offload: false
          fsdp_size: -1

        ref:
          fsdp_config:
            param_offload: true
            wrap_policy:
              min_num_params: 0
          log_prob_micro_batch_size: 2
          ulysses_sequence_parallel_size: 4

        rollout:
          rollout_standalone: true
          temperature: ${rollout_worker.temperature}
          dtype: bfloat16
          tensor_model_parallel_size: 2
          log_prob_micro_batch_size: 2
          n: ${data.rollout_repeat_n}
          use_weight_provider: false
